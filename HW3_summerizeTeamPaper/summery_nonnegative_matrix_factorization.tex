\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
\usepackage{amsmath}								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb,amsthm}
\usepackage[utf8]{inputenc}

%SetFonts

%SetFonts
\setlength{\parindent}{0pt}

\title{Nonnegative matrix factorization for interactive topic modeling and document clustering}
\author{Jody Shu}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\section{Optimization Framework for Nonnegative Matrix Factorization (NMF)}
%\subsection{}
\begin{Large}
In section 3, authors discuss block coordinate descent (BCD) and multiplicate updating (MU) methods to approximates a nonnegative matrix by the product of two low-rank nonegative matrices, i.e. A $\approx$ WH \ldots  (1).  MU is more inferior solutions than BCD due to its slow convergence which in the later section, the authors show the better resulting clustering by BCD algorithm.  

Basically, BCD is an algorithm takes turns solving W and H matrices and A is given as an input.  The follwings are the subproblems for the constraints of the BCD and are called nonnegatiity constrained least squares (NLS);therefore; the BCD has also been referred to the alternating nonnegatie least squares (ANLS).
\begin{center}
$\underset{W\geq0}{\mathrm{min}}\|H_{T}W_{T}-A_{T} \|^{\mathrm{2}}_{F} \ldots (8a)$\\
$\underset{H\geq0}{\mathrm{min}}\|WH-A\|^{\mathrm{2}}_{F}  \ldots (8b)$
\end{center}

One needs to initialize H or W first to solve (8a) and (8b) iteratively as stated in Algorithm 1 (Please see the original paper.).  The initialization of H or W matrices can be randomly and pick the solution with the smallest cost value, and other proposed method is to run the K-means with the cosine similarity as the distance function, and use the final outputs centroids as the initialization of W or H.  In addition, there are other methods to solve the subproblems.

The objective function of NMF is nonconvex, therefore, it's no gurarantee to find a local minimum, but can only find the stationarity of a limit point as stated in the following theorem 1.  

\textbf{Theorem 1}  If a minimum of each subproblem in (8) is attained at each step, every limit point of the sequence $\{(W,H)^{(i)}\}$ generated by the \textbf{BCD} framework is a stationary point of $\underset{H\geq0, W\geq0}{\mathrm{min}}\|A-WH\|^{\mathrm{2}}_{F}  \ldots (2)$

One should note that the minimum of each subproblem is not unique and this solution (stationary point), the Karush-Kuh-Tucher(KKT) condition is satisfied the follows:
\begin{flushleft}
$W\geq0, H\geq0$, \ldots (9a)\\
$\nabla f_W=2WHH^{T}-2AH^{T}\geq0$, $\nabla f_H=2W_{T}WH-2W_{T}A\geq0 \ldots (9b)$,
$W.\ast\nabla f_W= 0, H.\ast\nabla f_H= 0 \ldots (9c)$.\\
\end{flushleft}

As for the MU algorithm, it does not have the convergence property as \textbf{Theorem 1}, however, it is simple and easy to implement and it is more popular than BCD and the solutions for the subproblems (8) are not optimal.  The update rule of MU is similar to gradient descent algorithm with certain chosen step lengths for making sure the result is nonnegative.  Furthermore, the solutions of MU are denser than BCD framework, therefore, it's harder to explain the clustering results.

With the numerical analysis to find the solutions for the subproblems (8), a stop criteria between each objective function is set to be less than a pre-defined threshold $\varepsilon$ as follow:
$| f(W^{(i-1)},H^{(i-1)}) -f(W^{(i)},H^{(i)}) |\leq \varepsilon$\\

However, this is not a good method since the above-mentioned criteria may be satisfied before finding a stationary point.

Instead, the projected gradient matrices at the i-th iteration by $\nabla^{p} f^{\mathrm{(i)}}_{W} $ and $\nabla^{p} f^{\mathrm{(i)}}_{H} $ is defined as follows:\\
$\nabla(i)=\sqrt{\|\nabla^{p} f^{\mathrm{(i)}}_{W} \|^{\mathrm{2}}_{F} +\|\nabla^{p} f^{\mathrm{(i)}}_{H} \|^{\mathrm{2}}_{F} }\ldots (14)$\\
Where conditions (9) can be rephrased as $\nabla^{P} f_W= 0, \nabla^{P} f_H= 0\ldots(13)$,
Using equation (14), the stopping criterion becomes $\frac{\nabla(i)}{\nabla(1)}\leq\varepsilon \ldots (15)$
where $\nabla(1)$ is the first iteration of (W, H).

When there are hard clustering result, the authors impose extra constraints into the NMF formula (2) by adding two regularization terms to it.  The following are the two terms to be added to promote sparsity.

 $\phi(W)=\alpha\|W\|^{\mathrm{2}}_{F}, and\ \Psi(H)=\beta\sum_{i=1}^n\|H(:,i)\|^{\mathrm{2}}_{1}$
 where H(:,i) represents the i-th column of H.
 
Moreover, the authors introduce Weakly-Supervised NMF ($WS\-NMF$) which is semi-supervised algorithm which enhances the visual analytics environment for the users.  The formulation introduces $W_{r} \ and \ H_{r}$ which are the referenced matrices of W and H matrices respectively, and these referenced matrices are similar to W and H matrices.  In addition, there are $M_W\ and \ M_{H}$ for the diagonal mask/weight matrices.  The formula is as follows:\\
$f(W,H,D_H)=\underset{W,H,D_H}{\mathrm{min}}\|A-WH\|^{\mathrm{2}}_{F} \l+\|(W-W_r)M_W\|^{\mathrm{2}}_{F} +\|(H-H_rD_H)M_H\|^{\mathrm{2}}_{F}\ldots (19)$ where $D_H$ is a diagonal matrix.

To optimize formula (19), one has to use iterative method to update W, H(,i) and $D_H$ as follows:

W$\leftarrow \underset{W\geq0}{\mathrm{argmin}}\|[{\begin{array}{c}
H_T\\
M_W    
\end{array}}]\|W_T-[{\begin{array}{c}
A_T\\
M_WW^{\mathrm{T}}_{r}    
\end{array}}]\|^{\mathrm{2}}_{F}\ldots (20)$

H(:,i)$\leftarrow \underset{H(:,i)\geq0}{\mathrm{argmin}}\|[{\begin{array}{c}
W\\
M_{H}(i)I_{k}    
\end{array}}]\|H(:,i)-[{\begin{array}{c}
A(:,i)\\
M_{H}(i)D_{H}(i)H_{r}(:,i)    
\end{array}}]\|^{\mathrm{2}}_{F}\ldots (21)$

$D_{H}(i)=\leftarrow\begin{cases} \frac{H_{r}(:,i)^T \cdot H(:,i)}{\|H_{r}(:,i)\|^{\mathrm{2}}_{2}}.    &\text{if \(M_{H}(i)\neq0\)}\\ 0. &\text{otherwise}\ldots (21)\end{cases}$\\
where (:,i) indicates the i-th column of a matrix.

\section{Choosing the Number of Clusters}

The authors used the idea of a consensus matrix.  For a data set with n samples, the (i,j)-th entry of a consensus matrix $\tilde{C}\in\mathbb{R}^{n\times n}$ is the co-clustered frequency of the i-th and j-th samples over multiple runs of NMF.  Basically, there are T subsets generated by random sampling, each with sampling rate r, and run NMF algorithm on each subset with the same number of clusters k.  Define the elements in matrices $C^{(t)} \ and \ S^{(t)}$ as follows:\\

\begin{equation}\tag{23}
  c^{\mathrm{(t)}}_{ij} =\begin{cases}
    1, & \text{if $the\ i^{th}\ and \ the \ j^{th} \ documents\ belong\ to \ the \ same$}.\\
    &cluster\ using \ A_t;\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}

\begin{equation}\tag{24}
  s^{\mathrm{(t)}}_{ij} =\begin{cases}
    1, & \text{if $both the\ i^{th}\ and \ the \ j^{th} \ documents\ appear \ in \ A_t$}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}

Obviously, if $c^{\mathrm{(t)}}_{ij} =1 \Rightarrow s^{\mathrm{(t)}}_{ij} =1$

The element of consensus matrix $\tilde{c}_{ij}=\frac{\sum_{t=1}^T c^{\mathrm{(t)}}_{ij} }{\sum_{t=1}^T s^{\mathrm{(t)}}_{ij} }\ldots(25)$.  And the dispersion coefficient $\rho$ becomes: \\

\hspace{120pt} $\rho=\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n4(\tilde{c}_{ij}-0.5)^2\ldots(26)$\\
\\where the $0\leq\rho<1$.  The number of clusters is chosen to be the maximum of $\rho$.


\end{Large}










\end{document}  