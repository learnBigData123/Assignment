\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
\usepackage{amsmath}								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb,amsthm}
\usepackage[utf8]{inputenc}

%SetFonts

%SetFonts
\setlength{\parindent}{0pt}

\title{Nonnegative matrix factorization for interactive topic modeling and document clustering}
\author{Sarah Lu, Jody Shu, Jashaina Thomas}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

Nonnegative matrix factorization (NMF) approximates a nonnegative matrix by the product of two low-rank nonnegative matrices. Since it gives a semantically meaningful result that is easily interpretable in clustering applications, NMF has been widely used as a clustering method especially for document data, and as a topic modeling method. These notes focus on the broad spectrum of NMF in the context of clustering and topic modeling.

\section{Introduction to Nonnegative Matrix Factorization}

Nonnegative matrix factorization (NMF) is a dimension reduction method and factor analysis method related to low-rank approximations of matrices in which the low-rank factor matrices are constrained to have only nonnegative elements.

Suppose a nonnegative matrix $A \in \mathbb{R}^{mxn} $ is given. When the desired lower dimension is k, the goal of NMF is to find the two matrices $W \in \mathbb{R}^{mxk} $ k and $H \in \mathbb{R}^{kxn} $ n having only nonnegative entries such that

\begin{center}
$A  \approx W H$.

\end{center}

Each data point, which is represented as the column of A, can be approximated by an additive combination of the nonnegative basis vectors, which are represented as the columns of W. k is assumed to satisfy $k < min\{m, n\}$. The matrices W and H are found by solving an optimization problem defined with the Frobenius norm (a distance measure between two given matrices).
 
 \begin{center}
 $min\{W\ge0,H\ge0\}f(W,H)=||A-WH||^2_F$
 \end{center}

All entries of W and H are nonnegative. Because of this, the result of NMF can be viewed as document clustering and topic modeling results directly.

%\subsection{}

\section{Nonnegative Matrix Factorization for Clustering}

Consider the low-rank approximation where $A \in \mathbb{R}^{mxn}_+$ , $W \in \mathbb{R}^{mxk}_+$ ,$H \in \mathbb{R}^{kxn}_+$ , and $k << min(m,n)$ is the pre-specified lower rank. The columns of A represent n data points in an m dimensional space. Each column of H is the k-dimensional representation of a data point. If we can use H to derive an assignment of the n data points into k groups, clustering can be viewed as a special type of dimension reduction.


An example of dimension reduction is NMF:

 \begin{center}
 $min\{W\ge0,H\ge0\}f(W,H)=||A-WH||^2_F$
 \end{center}

The columns of W provide the basis of a latent k-dimensional space, and the columns of the second factor H provide the representation of $a_1,...,a_n$ in the latent space. The columns of W are interpreted as k cluster representatives, and the i-th column of H contains the soft clustering membership of the i-th data point for the k clusters. In document clustering and topic modeling, the basis vectors in W represent k topics, and the coefficients in the i-th column of H indicate the topic proportions for $a_i$, the i-th document.To obtain a hard clustering result, we simply choose the topic with the largest weight, i.e., the largest element in each column of H.


If $k < rank(A)$, the columns of W are linearly independent due to $rank(A) \le nonnegative-rank(A)$. Therefore, NMF performs well when different clusters correspond to linearly independent vectors.

The success of NMF as a clustering method depends on the underlying data set, and its greatest success has been in the area of document clustering. Recently, NMF has been applied to topic modeling, a task related to document clustering, and achieved satisfactory results. In a document data set, data points are often represented as unit-length vectors and embedded in a linear subspace. For a term-document matrix A, a basis vector $w_j$ is interpreted as the keyword-wise distribution of a single topic. When these distributions of k topics are linearly independent, which is usually the case, NMF can properly extract the ground-truth clusters determined by the true cluster labels.

The output types include: a keyword-wise topic representation (the columns of W), and a topic-wise document representation (the columns of H). 

\section{Optimization Framework for Nonnegative Matrix Factorization (NMF)}
%\subsection{}
In section 3, authors discuss block coordinate descent (BCD) and multiplicate updating (MU) methods to approximates a nonnegative matrix by the product of two low-rank nonegative matrices, i.e. A $\approx$ WH \ldots  (1).  MU is more inferior solutions than BCD due to its slow convergence which in the later section, the authors show the better resulting clustering by BCD algorithm.  

Basically, BCD is an algorithm takes turns solving W and H matrices and A is given as an input.  The follwings are the subproblems for the constraints of the BCD and are called nonnegatiity constrained least squares (NLS);therefore; the BCD has also been referred to the alternating nonnegatie least squares (ANLS).
\begin{center}
$\underset{W\geq0}{\mathrm{min}}\|H_{T}W_{T}-A_{T} \|^{\mathrm{2}}_{F} \ldots (8a)$\\
$\underset{H\geq0}{\mathrm{min}}\|WH-A\|^{\mathrm{2}}_{F}  \ldots (8b)$
\end{center}

One needs to initialize H or W first to solve (8a) and (8b) iteratively as stated in Algorithm 1 (Please see the original paper.).  The initialization of H or W matrices can be randomly and pick the solution with the smallest cost value, and other proposed method is to run the K-means with the cosine similarity as the distance function, and use the final outputs centroids as the initialization of W or H.  In addition, there are other methods to solve the subproblems.

The objective function of NMF is nonconvex, therefore, it's no gurarantee to find a local minimum, but can only find the stationarity of a limit point as stated in the following theorem 1.  

\textbf{Theorem 1}  If a minimum of each subproblem in (8) is attained at each step, every limit point of the sequence $\{(W,H)^{(i)}\}$ generated by the \textbf{BCD} framework is a stationary point of $\underset{H\geq0, W\geq0}{\mathrm{min}}\|A-WH\|^{\mathrm{2}}_{F}  \ldots (2)$

One should note that the minimum of each subproblem is not unique and this solution (stationary point), the Karush-Kuh-Tucher(KKT) condition is satisfied the follows:
\begin{flushleft}
$W\geq0, H\geq0$, \ldots (9a)\\
$\nabla f_W=2WHH^{T}-2AH^{T}\geq0$, $\nabla f_H=2W_{T}WH-2W_{T}A\geq0 \ldots (9b)$,
$W.\ast\nabla f_W= 0, H.\ast\nabla f_H= 0 \ldots (9c)$.\\
\end{flushleft}

As for the MU algorithm, it does not have the convergence property as \textbf{Theorem 1}, however, it is simple and easy to implement and it is more popular than BCD and the solutions for the subproblems (8) are not optimal.  The update rule of MU is similar to gradient descent algorithm with certain chosen step lengths for making sure the result is nonnegative.  Furthermore, the solutions of MU are denser than BCD framework, therefore, it's harder to explain the clustering results.

With the numerical analysis to find the solutions for the subproblems (8), a stop criteria between each objective function is set to be less than a pre-defined threshold $\varepsilon$ as follow:
$| f(W^{(i-1)},H^{(i-1)}) -f(W^{(i)},H^{(i)}) |\leq \varepsilon$\\

However, this is not a good method since the above-mentioned criteria may be satisfied before finding a stationary point.

Instead, the projected gradient matrices at the i-th iteration by $\nabla^{p} f^{\mathrm{(i)}}_{W} $ and $\nabla^{p} f^{\mathrm{(i)}}_{H} $ is defined as follows:\\
$\nabla(i)=\sqrt{\|\nabla^{p} f^{\mathrm{(i)}}_{W} \|^{\mathrm{2}}_{F} +\|\nabla^{p} f^{\mathrm{(i)}}_{H} \|^{\mathrm{2}}_{F} }\ldots (14)$\\
Where conditions (9) can be rephrased as $\nabla^{P} f_W= 0, \nabla^{P} f_H= 0\ldots(13)$,
Using equation (14), the stopping criterion becomes $\frac{\nabla(i)}{\nabla(1)}\leq\varepsilon \ldots (15)$
where $\nabla(1)$ is the first iteration of (W, H).

When there are hard clustering result, the authors impose extra constraints into the NMF formula (2) by adding two regularization terms to it.  The following are the two terms to be added to promote sparsity.

 $\phi(W)=\alpha\|W\|^{\mathrm{2}}_{F}, and\ \Psi(H)=\beta\sum_{i=1}^n\|H(:,i)\|^{\mathrm{2}}_{1}$
 where H(:,i) represents the i-th column of H.
 
Moreover, the authors introduce Weakly-Supervised NMF ($WS\-NMF$) which is semi-supervised algorithm which enhances the visual analytics environment for the users.  The formulation introduces $W_{r} \ and \ H_{r}$ which are the referenced matrices of W and H matrices respectively, and these referenced matrices are similar to W and H matrices.  In addition, there are $M_W\ and \ M_{H}$ for the diagonal mask/weight matrices.  The formula is as follows:\\
$f(W,H,D_H)=\underset{W,H,D_H}{\mathrm{min}}\|A-WH\|^{\mathrm{2}}_{F} \l+\|(W-W_r)M_W\|^{\mathrm{2}}_{F} +\|(H-H_rD_H)M_H\|^{\mathrm{2}}_{F}\ldots (19)$ where $D_H$ is a diagonal matrix.

To optimize formula (19), one has to use iterative method to update W, H(,i) and $D_H$ as follows:

W$\leftarrow \underset{W\geq0}{\mathrm{argmin}}\|[{\begin{array}{c}
H_T\\
M_W    
\end{array}}]\|W_T-[{\begin{array}{c}
A_T\\
M_WW^{\mathrm{T}}_{r}    
\end{array}}]\|^{\mathrm{2}}_{F}\ldots (20)$

H(:,i)$\leftarrow \underset{H(:,i)\geq0}{\mathrm{argmin}}\|[{\begin{array}{c}
W\\
M_{H}(i)I_{k}    
\end{array}}]\|H(:,i)-[{\begin{array}{c}
A(:,i)\\
M_{H}(i)D_{H}(i)H_{r}(:,i)    
\end{array}}]\|^{\mathrm{2}}_{F}\ldots (21)$

$D_{H}(i)=\leftarrow\begin{cases} \frac{H_{r}(:,i)^T \cdot H(:,i)}{\|H_{r}(:,i)\|^{\mathrm{2}}_{2}}.    &\text{if \(M_{H}(i)\neq0\)}\\ 0. &\text{otherwise}\ldots (21)\end{cases}$\\
where (:,i) indicates the i-th column of a matrix.

\section{Choosing the Number of Clusters}

The authors used the idea of a consensus matrix.  For a data set with n samples, the (i,j)-th entry of a consensus matrix $\tilde{C}\in\mathbb{R}^{n\times n}$ is the co-clustered frequency of the i-th and j-th samples over multiple runs of NMF.  Basically, there are T subsets generated by random sampling, each with sampling rate r, and run NMF algorithm on each subset with the same number of clusters k.  Define the elements in matrices $C^{(t)} \ and \ S^{(t)}$ as follows:\\

\begin{equation}\tag{23}
  c^{\mathrm{(t)}}_{ij} =\begin{cases}
    1, & \text{if $the\ i^{th}\ and \ the \ j^{th} \ documents\ belong\ to \ the \ same$}.\\
    &cluster\ using \ A_t;\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}

\begin{equation}\tag{24}
  s^{\mathrm{(t)}}_{ij} =\begin{cases}
    1, & \text{if $both the\ i^{th}\ and \ the \ j^{th} \ documents\ appear \ in \ A_t$}.\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation}

Obviously, if $c^{\mathrm{(t)}}_{ij} =1 \Rightarrow s^{\mathrm{(t)}}_{ij} =1$

The element of consensus matrix $\tilde{c}_{ij}=\frac{\sum_{t=1}^T c^{\mathrm{(t)}}_{ij} }{\sum_{t=1}^T s^{\mathrm{(t)}}_{ij} }\ldots(25)$.  And the dispersion coefficient $\rho$ becomes: \\

\hspace{120pt} $\rho=\frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n4(\tilde{c}_{ij}-0.5)^2\ldots(26)$\\
\\where the $0\leq\rho<1$.  The number of clusters is chosen to be the maximum of $\rho$.

\section{Experimental Results}
%\subsection{}
In section 5, authors  present the empirical evidences that support NMF as a successful document clustering and topic modeling method. They compare the clustering quality between K-means and NMF; Within the NMF algorithms, they compare the multiplicative updating (MU) algorithm and the alternating nonnegative least squares (ANLS) algorithm in terms of their clustering quality and convergence behavior, as well as sparseness and consistency in the solution.  

The data are described as follows:
\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 Data set & Terms & Documents & Ground-truth clusters \\ [0.5ex] 
 \hline\hline
 TDT2 & 26,618 & 8,741 & 20 \\ 
 \hline
 Reuters & 12,998 & 8,095 & 20 \\
 \hline
 20 Newsgroups & 36,568 & 18,221 & 20 \\
 \hline
 RCV1 & 20,338 & 15,168 & 40 \\
 \hline
 NIPS14-16 & 17,583 & 420 & 9 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

They  process each term-document matrix $A$ in two steps. First, they normalize each column of $A$ to have a unit $L2$-norm. Conceptually, this makes all the documents have equal lengths. Next, they compute the normalized-cut weighted version of $A$:
\begin{center}
$D = diag(A^{T}A1_n), A\leftarrow AD^{-1/2}$
\end{center}

For K-means clustering, they used the standard K-means with Euclidean distances, through a batch-update phase and a more time-consuming online-update phase in Matlab.

For the ANLS algorithm for NMF, they used the block principal pivoting algorithm to solve the NLS subproblems and the stopping criterion was $\varepsilon = 10^{-4}$. For the MU algorithm for NMF, they used another stopping criterion:
\begin{center}
$\|H^{i-1}-H^{i}\|_{F}/\|H^{i}\|_{F}\leq\varepsilon$
\end{center}

\textbf Clustering quality: Normalized Mutual information (NMI) is calculated by:
\begin{center}
NMI $= \frac{I(C_{ground_truth},C_{computed})}{[H(C_{ground_truth})+H(C_{computed})]/2}=\frac{\sum_{h,l} n_{h,l}\log \frac{n\cdotn_{h,l}}{n_{h}n_{l}}}{(\sum_{h} n_{h}\log \frac{n_{h}}{n}+\sum_{l} n_{l}\log \frac{n_{l}}{n})/2}$
\end{center}

\section{UTOPIAN: User-driven Topic Modeling via Interactive NMF}

In this section, the authors present a visual analytics system called UTOPIAN (User-driven Topic Modeling Based on Interactive NMF). UTOPIAN provides a visual overview of the NMF topic modeling result. Beyond the visual exploration of the topic modeling result in a passive manner, UTOPIAN provides various interaction capabilities that can actively incorporate user inputs to topic modeling processes.\\
Topic keyword refinement: This interaction allows users to change the weights corresponding to keywords so that the meaning of the topic can be refined.\\
Topic merging: This interaction merges multiple topics into one.\\
Topic splitting: It splits a particular topic into the two topics. To guide this splitting process, users can assign the reference information for the two topics.\\
Document-induced topic creation: This interaction creates a new topic by using user-selected documents as seed documents.\\
Keyword-induced topic creation. It creates a new topic via user-selected keywords. For instance, given the summary of topics as their representative keywords, users might want to explore more detailed (sub-)topics about particular keywords.

\section{Conclusions and Future Directions}
In this paper, the authors have presented nonnegative matrix factorization (NMF) for document clustering and topic modeling. They have first introduced the NMF formulation and its applications to clustering. Next, they have presented the flexible algorithmic framework based on block coordinate descent (BCD) as well as its convergence property and stopping criterion. Based on the BCD framework, they discussed two important extensions for clustering, the sparse and the weakly-supervised NMF,and their method to determine the number of clusters. Experimental results on various real-world document data sets show the advantage of their NMF algorithm in terms of clustering quality, convergence behavior, sparseness, and consistency. Finally, they presented a visual analytics system called UTOPIAN for interactive visual clustering and topic modeling and demonstrated its interaction capabilities such as topic splitting/merging as well as keyword-/document-induced topic creation.


\end{document}  