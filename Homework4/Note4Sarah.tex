\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{HW4-Lecture notes}
\author{ylu20 }
\date{February 2020}

\begin{document}

\maketitle

\section{Kernel methods}
\subsection{Introduction}
Kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations in datasets. Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the "kernel trick". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.
\subsection{Kernel matrix}
\begin{equation}
\Phi(\vec{x})^\top = 
\begin{bmatrix}
\Phi(x_{1})^\top\\
\Phi(x_{2})^\top\\
\vdots
\Phi(x_{n})^\top
\end{bmatrix}
\end{equation}

\begin{equation}
\begin{bmatrix}
\Phi(x_{1})^\top\\
\Phi(x_{2})^\top\\
\vdots
\Phi(x_{n})^\top
\end{bmatrix}
\begin{bmatrix}
\Phi(x_{1}) & \Phi(x_{2}) & \cdots & \Phi(x_{n})
\end{bmatrix}=
\begin{bmatrix}
\Phi(x_{1})^\top\Phi(x_{1}) & \Phi(x_{1})^\top\Phi(x_{2}) & \cdots \\
\Phi(x_{2})^\top\Phi(x_{1})\\
\vdots
\end{bmatrix}
\end{equation}
Property: This matrix is positive semidefinite! $\forall \vec{u}$, 
\begin{equation}
\vec{u}^\top\Phi(x)^\top\Phi(x)\vec{u} = (\Phi(x)\vec{u})^\top(\Phi(x)\vec{u}) = \|\Phi(x)\vec{u}\|^2 \geq 0
\end{equation}
Example: $(x^{i}, y^{i})$
\begin{equation}
K(x,y) = (2<x,y> + 5)^2
\end{equation}
\end{document}
